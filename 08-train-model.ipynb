{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.compose import make_column_selector\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import subprocess\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "\n",
    "def load_train_data(split=True):\n",
    "    target = \"Survived\"\n",
    "    data = pd.read_csv(\"./train.csv\", index_col=\"PassengerId\")\n",
    "    print(\"load_train_data: done\")\n",
    "    \n",
    "    if split:\n",
    "        return split_features_target(data, target)\n",
    "    else:\n",
    "        return data\n",
    "    \n",
    "def split_features_target(df, target=\"Survived\"):\n",
    "    features = [column for column in df.columns if not column == target]\n",
    "    return df[features], df[target]\n",
    "\n",
    "class DFSimpleImputer(SimpleImputer):\n",
    "    def transform(self, X, y=None):\n",
    "        return pd.DataFrame(super().transform(X), columns=X.columns)\n",
    "\n",
    "class DFOneHotEncoder(OneHotEncoder):\n",
    "    def transform(self, X, y=None):\n",
    "        column_names = X.columns\n",
    "        X_transformed = super().transform(X)\n",
    "        return pd.DataFrame.sparse.from_spmatrix(X_transformed, columns=self.get_feature_names(column_names))\n",
    "    \n",
    "# Save Kaggle submission file\n",
    "def submission_df(y_pred):\n",
    "    X_test = load_x_test()\n",
    "    return pd.DataFrame(y_pred, index=X_test.index, columns=[\"Survived\"])\n",
    "\n",
    "def save_submission_file(y_pred, filename):\n",
    "    df = submission_df(y_pred)\n",
    "    path = \"./\" + filename\n",
    "\n",
    "    try:\n",
    "        df.to_csv(path)\n",
    "    except Exception:\n",
    "        print(\"Couldnâ€™t save submission.\")\n",
    "    else:\n",
    "        print(\"Submission saved.\")\n",
    "        \n",
    "# Submit score to Kaggle\n",
    "def submit_predictions(y_pred, filename, message):\n",
    "    save_submission_file(y_pred, filename)\n",
    "\n",
    "    completed_process = subprocess.run(\n",
    "        [\n",
    "            \"kaggle\",\n",
    "            \"competitions\",\n",
    "            \"submit\",\n",
    "            \"-c\",\n",
    "            \"titanic\",\n",
    "            \"-f\",\n",
    "            filename,\n",
    "            \"-m\",\n",
    "            message\n",
    "        ], \n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    \n",
    "    print(completed_process.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sex pipeline\n",
    "\n",
    "sex_pipeline = Pipeline([\n",
    "    ('imputer', DFSimpleImputer(strategy=\"constant\", fill_value=\"Unknown\")),\n",
    "    ('one_hot_encoder', DFOneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "\n",
    "# Embarked pipeline\n",
    "\n",
    "embarked_pipeline = Pipeline([\n",
    "    ('imputer', DFSimpleImputer(strategy=\"constant\", fill_value=\"Unknown\")),\n",
    "    ('one_hot_encoder', DFOneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "\n",
    "# Age pipeline\n",
    "    \n",
    "def generate_age_derived_features(X):\n",
    "    has_age = X[\"Age\"].apply(lambda x: 0 if pd.isnull(x) else 1).rename(\"HasAge\")\n",
    "    is_young_child = X[\"Age\"].apply(lambda age: 1 if age < 10 else 0).rename(\"IsYoungChild\")\n",
    "    is_infant = X[\"Age\"].apply(lambda age: 1 if age < 1 else 0).rename(\"IsInfant\")\n",
    "    return pd.concat([X, has_age, is_young_child, is_infant], axis=\"columns\")\n",
    "\n",
    "def discretize_age(X):\n",
    "    bins = [0.0, 10.0, 20.0, 30.0, 40.0, 50.0, 60.0, 70.0, np.inf]\n",
    "    labels = [\"0+\", \"10+\", \"20+\", \"30+\", \"40+\", \"50+\", \"60+\", \"70+\"]\n",
    "\n",
    "    age_group = pd.cut(X[\"Age\"], bins=bins, labels=labels, include_lowest=True)\n",
    "    age_group = age_group.rename(\"AgeGroup\")\n",
    "    \n",
    "    result = pd.concat([X, age_group], axis=\"columns\")    \n",
    "    return result\n",
    "\n",
    "def drop_age(X):\n",
    "    X = X.drop(\"Age\", axis=\"columns\")\n",
    "    return X\n",
    "\n",
    "age_pipeline = Pipeline([\n",
    "    (\"age_transformer\", FunctionTransformer(generate_age_derived_features)),\n",
    "    ('impute', DFSimpleImputer(strategy=\"mean\")),\n",
    "    (\"discretize\", FunctionTransformer(discretize_age)),\n",
    "    (\"drop\", FunctionTransformer(drop_age)),\n",
    "    ('one_hot_encoder', DFOneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "\n",
    "# Dropping attributes\n",
    "\n",
    "def attributes_dropper(df):\n",
    "    result_df = df.drop(\"Name\", axis=\"columns\")\n",
    "    result_df = result_df.drop(\"Ticket\", axis=\"columns\")\n",
    "    return result_df\n",
    "\n",
    "attributes_dropper_transformer = FunctionTransformer(attributes_dropper)\n",
    "\n",
    "\n",
    "# Fare pipeline\n",
    "\n",
    "fare_pipeline = Pipeline([\n",
    "    ('impute_missing', DFSimpleImputer(strategy=\"mean\")),\n",
    "    (\"log_transform\", FunctionTransformer(np.log1p)),\n",
    "    ('standard_scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "\n",
    "# Cabin pipeline\n",
    "\n",
    "def generate_cabin_derived_features(df):\n",
    "    has_cabin = df[\"Cabin\"].apply(lambda x: 0 if pd.isnull(x) else 1).rename(\"HasCabin\")\n",
    "    cabin_letter = df[\"Cabin\"].apply(lambda string: string[0] if pd.notnull(string) else \"U\")\n",
    "    cabin_letter = cabin_letter.rename(\"CabinLetter\")\n",
    "\n",
    "    return pd.concat([df, has_cabin, cabin_letter], axis=\"columns\")\n",
    "\n",
    "def drop_cabin(df):\n",
    "    result = df.drop(\"Cabin\", axis=\"columns\")\n",
    "    return result\n",
    "\n",
    "cabin_pipeline = Pipeline([\n",
    "    (\"generate_derived_features\", FunctionTransformer(generate_cabin_derived_features)),\n",
    "    ('impute', DFSimpleImputer(strategy=\"constant\", fill_value=\"Unknown\")),\n",
    "    (\"drop\", FunctionTransformer(drop_cabin)),\n",
    "    ('one_hot_encoder', DFOneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_ticket_pipe = Pipeline([\n",
    "    ('imputer', DFSimpleImputer(strategy=\"constant\", fill_value=\"Unknown\")),\n",
    "    ('one_hot_encoder', DFOneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_pipeline = ColumnTransformer([\n",
    "        (\"sex\", sex_pipeline, [\"Sex\"]),\n",
    "        (\"embarked\", embarked_pipeline, [\"Embarked\"]),\n",
    "        (\"age\", age_pipeline, [\"Age\"]),\n",
    "        (\"name_ticket_pipe\", name_ticket_pipe, [\"Name\", \"Ticket\"]),\n",
    "        (\"fare\", fare_pipeline, [\"Fare\"]),\n",
    "        (\"cabin\", cabin_pipeline, [\"Cabin\"]),\n",
    "    ],\n",
    "    remainder=\"passthrough\"\n",
    ")\n",
    "\n",
    "model = svm.LinearSVC(random_state=42, max_iter=10000)\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"preprocessing\", preprocessing_pipeline),\n",
    "    (\"model\", model)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_train_data: done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8047266336074321"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, y_train = load_train_data()\n",
    "scores = cross_val_score(pipe, X_train, y_train)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_train_data: done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8047266336074321"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, y_train = load_train_data()\n",
    "scores = cross_val_score(pipe, X_train, y_train)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_train_data: done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8339024543343168"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, y_train = load_train_data()\n",
    "scores = cross_val_score(pipe, X_train, y_train)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission saved.\n",
      "Successfully submitted to Titanic: Machine Learning from Disaster\n"
     ]
    }
   ],
   "source": [
    "def load_x_test():\n",
    "    return pd.read_csv(\"./test.csv\", index_col=\"PassengerId\")\n",
    "\n",
    "X_test = load_x_test()\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "y_pred = pipe.predict(X_test)\n",
    "\n",
    "# submit_predictions(\n",
    "#     y_pred, \n",
    "#     \"04-preprocessing-pipeline.csv\",\n",
    "#     \"Implemented own custom preprocessing pipeline. LinearSVC.\"\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "kaggle"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
